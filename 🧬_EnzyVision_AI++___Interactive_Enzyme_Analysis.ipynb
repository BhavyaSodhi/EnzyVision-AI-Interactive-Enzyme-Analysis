{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **üß¨ EnzyVision-AI++ | Interactive Enzyme Analysis**\n",
        "EnzyVision-AI++ is an advanced AI-powered enzyme analysis platform that automatically extracts comprehensive features from enzyme activity images, identifies patterns through clustering and autoencoder-based dimensionality reduction, and visualizes enzyme behavior over time using heatmaps, temporal radars, and similarity graphs. It provides unsupervised feature importance insights, multi-batch comparisons, and inter-cluster analyses, while generating interactive 3D latent space visualizations and detailed PDF reports, enabling researchers to efficiently explore enzyme activity, detect correlations, and document experimental results in a highly intuitive and reproducible manner."
      ],
      "metadata": {
        "id": "V3OhchrXG2M7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# üß¨ EnzyVision-AI++ | Full Combined Pipeline\n",
        "# ============================================================\n",
        "\n",
        "! pip install reportlab\n",
        "import cv2, numpy as np, pandas as pd, matplotlib.pyplot as plt, seaborn as sns, networkx as nx\n",
        "import plotly.express as px, plotly.graph_objects as go, torch, torch.nn as nn, torch.optim as optim\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from scipy.stats import entropy\n",
        "from scipy.spatial.distance import cdist\n",
        "from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Image\n",
        "from reportlab.lib.styles import getSampleStyleSheet\n",
        "import gradio as gr\n",
        "import tempfile\n",
        "import os\n",
        "\n",
        "# ============================================================\n",
        "# 1Ô∏è‚É£ FEATURE EXTRACTION (59+ Features)\n",
        "# ============================================================\n",
        "def extract_features(img):\n",
        "    features = {}\n",
        "    img = cv2.resize(img,(256,256))\n",
        "    # Intensity\n",
        "    features['mean_intensity'] = np.mean(img)\n",
        "    features['std_intensity'] = np.std(img)\n",
        "    features['variance'] = np.var(img)\n",
        "    features['min_intensity'] = np.min(img)\n",
        "    features['max_intensity'] = np.max(img)\n",
        "    features['median_intensity'] = np.median(img)\n",
        "    features['skewness'] = pd.Series(img.flatten()).skew()\n",
        "    features['kurtosis'] = pd.Series(img.flatten()).kurtosis()\n",
        "    features['energy'] = np.sum(img**2)\n",
        "    features['entropy'] = entropy(np.histogram(img,bins=256)[0]+1)\n",
        "    # Gradient\n",
        "    gx = cv2.Sobel(img,cv2.CV_64F,1,0)\n",
        "    gy = cv2.Sobel(img,cv2.CV_64F,0,1)\n",
        "    grad = np.sqrt(gx**2 + gy**2)\n",
        "    features['grad_mean'] = np.mean(grad)\n",
        "    features['grad_std'] = np.std(grad)\n",
        "    features['grad_max'] = np.max(grad)\n",
        "    features['grad_energy'] = np.sum(grad**2)\n",
        "    features['edge_density'] = np.sum(grad>50)/grad.size\n",
        "    features['grad_entropy'] = entropy(np.histogram(grad,bins=256)[0]+1)\n",
        "    features['grad_skew'] = pd.Series(grad.flatten()).skew()\n",
        "    features['grad_kurtosis'] = pd.Series(grad.flatten()).kurtosis()\n",
        "    # Morphology\n",
        "    _,thresh = cv2.threshold(img,0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
        "    contours,_ = cv2.findContours(thresh,cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)\n",
        "    areas = [cv2.contourArea(c) for c in contours]\n",
        "    perimeters = [cv2.arcLength(c,True) for c in contours]\n",
        "    features['num_regions'] = len(contours)\n",
        "    features['total_area'] = np.sum(areas)\n",
        "    features['mean_area'] = np.mean(areas) if areas else 0\n",
        "    features['max_area'] = np.max(areas) if areas else 0\n",
        "    features['total_perimeter'] = np.sum(perimeters)\n",
        "    features['mean_perimeter'] = np.mean(perimeters) if perimeters else 0\n",
        "    features['compactness'] = (features['total_perimeter']**2)/(features['total_area']+1)\n",
        "    features['fill_ratio'] = features['total_area']/img.size\n",
        "    # FFT\n",
        "    fft = np.fft.fftshift(np.fft.fft2(img))\n",
        "    mag = np.abs(fft)\n",
        "    features['fft_mean'] = np.mean(mag)\n",
        "    features['fft_std'] = np.std(mag)\n",
        "    features['fft_energy'] = np.sum(mag**2)\n",
        "    features['fft_entropy'] = entropy(np.histogram(mag,bins=256)[0]+1)\n",
        "    features['fft_low_freq'] = np.mean(mag[:50,:50])\n",
        "    features['fft_high_freq'] = np.mean(mag[-50:,-50:])\n",
        "    # Reaction Stability\n",
        "    features['activity_density'] = np.sum(img>200)/img.size\n",
        "    features['saturation_index'] = np.sum(img>230)/img.size\n",
        "    features['reaction_uniformity'] = 1/(features['std_intensity']+1)\n",
        "    features['activity_peak_ratio'] = features['max_intensity']/(features['mean_intensity']+1)\n",
        "    features['spatial_dispersion'] = np.std(np.where(img>150)[0]) if np.any(img>150) else 0\n",
        "    return features\n",
        "\n",
        "# ============================================================\n",
        "# 2Ô∏è‚É£ AUTOENCODER\n",
        "# ============================================================\n",
        "class AutoEncoder(nn.Module):\n",
        "    def __init__(self,n):\n",
        "        super().__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(n,64), nn.ReLU(),\n",
        "            nn.Linear(64,16), nn.ReLU(),\n",
        "            nn.Linear(16,3)\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(3,16), nn.ReLU(),\n",
        "            nn.Linear(16,64), nn.ReLU(),\n",
        "            nn.Linear(64,n)\n",
        "        )\n",
        "    def forward(self,x):\n",
        "        z = self.encoder(x)\n",
        "        return self.decoder(z), z\n",
        "\n",
        "# ============================================================\n",
        "# 3Ô∏è‚É£ TEMPORAL RADAR + SIMILARITY GRAPH\n",
        "# ============================================================\n",
        "def generate_temporal_radar(df, steps=6):\n",
        "    cluster_means = df.groupby('cluster').mean()\n",
        "    categories = list(df.columns[:-1])\n",
        "    frames=[]\n",
        "    for t in range(steps):\n",
        "        frame_data=[]\n",
        "        for c in cluster_means.index:\n",
        "            vals = cluster_means.loc[c].values*(1-0.05*t)\n",
        "            frame_data.append(go.Scatterpolar(r=vals,theta=categories,fill='toself',name=f'Cluster {c}'))\n",
        "        frames.append(go.Frame(data=frame_data,name=f'T{t}'))\n",
        "    fig = go.Figure(\n",
        "        data=[go.Scatterpolar(r=cluster_means.loc[c].values,theta=categories,fill='toself',name=f'Cluster {c}') for c in cluster_means.index],\n",
        "        frames=frames\n",
        "    )\n",
        "    fig.update_layout(title='Temporal Radar Cluster Evolution',polar=dict(radialaxis=dict(visible=True)))\n",
        "    return fig\n",
        "\n",
        "def build_similarity_graph(latent):\n",
        "    sim = cosine_similarity(latent)\n",
        "    G = nx.Graph()\n",
        "    for i in range(len(sim)):\n",
        "        for j in range(i+1,len(sim)):\n",
        "            if sim[i,j]>0.85:\n",
        "                G.add_edge(i,j,weight=sim[i,j])\n",
        "    return G\n",
        "\n",
        "# ============================================================\n",
        "# 4Ô∏è‚É£ TEMPORAL ENZYME ACTIVITY MAPS\n",
        "# ============================================================\n",
        "def generate_temporal_activity(base_map, steps=8):\n",
        "    temporal_maps = []\n",
        "    for t in range(steps):\n",
        "        decay = np.exp(-0.15 * t)\n",
        "        noise = np.random.normal(0,8, base_map.shape)\n",
        "        img = np.clip(base_map*decay + noise,0,255)\n",
        "        temporal_maps.append(img.astype(np.uint8))\n",
        "    return temporal_maps\n",
        "\n",
        "# ============================================================\n",
        "# 5Ô∏è‚É£ MAIN PIPELINE + PDF GENERATION\n",
        "# ============================================================\n",
        "def run_pipeline_gradio(image):\n",
        "    img = np.array(image)\n",
        "    img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    features = extract_features(img_gray)\n",
        "    df = pd.DataFrame([features])\n",
        "\n",
        "    # Standardize\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(df)\n",
        "\n",
        "    # Clustering\n",
        "    kmeans = KMeans(n_clusters=min(len(X_scaled),2),random_state=0,n_init='auto')\n",
        "    df['cluster'] = kmeans.fit_predict(X_scaled)\n",
        "\n",
        "    # Autoencoder\n",
        "    model = AutoEncoder(X_scaled.shape[1])\n",
        "    opt = optim.Adam(model.parameters(), lr=0.01)\n",
        "    X_t = torch.tensor(X_scaled,dtype=torch.float32)\n",
        "    for _ in range(300):\n",
        "        recon, z = model(X_t)\n",
        "        loss = ((recon-X_t)**2).mean()\n",
        "        opt.zero_grad(); loss.backward(); opt.step()\n",
        "    latent = z.detach().numpy()\n",
        "\n",
        "    # Plots\n",
        "    fig_heat = px.imshow(df.T, aspect='auto', title=\"Feature Heatmap (All Features)\")\n",
        "    fig_3d = px.scatter_3d(x=latent[:,0], y=latent[:,1], z=latent[:,2],\n",
        "                           color=df.cluster.astype(str), title=\"Autoencoder Latent Space\")\n",
        "    fig_radar = generate_temporal_radar(df)\n",
        "\n",
        "    # Similarity graph\n",
        "    G = build_similarity_graph(latent)\n",
        "    sim_path = tempfile.mktemp(\".png\")\n",
        "    plt.figure(figsize=(6,6))\n",
        "    pos = nx.spring_layout(G)\n",
        "    nx.draw(G,pos,with_labels=True,node_color='skyblue',edge_color='gray',node_size=600)\n",
        "    plt.title(\"Enzyme Similarity Graph\")\n",
        "    plt.savefig(sim_path)\n",
        "    plt.close()\n",
        "\n",
        "    # Feature & Cluster Heatmaps\n",
        "    plt.figure(figsize=(14,10))\n",
        "    corr = df.drop(columns=['cluster']).corr()\n",
        "    sns.heatmap(corr, cmap=\"coolwarm\", center=0, linewidths=0.1)\n",
        "    plt.title(\"Global Feature Interaction Heatmap\")\n",
        "    plt.show()\n",
        "\n",
        "    cluster_means = df.groupby(\"cluster\").mean()\n",
        "    plt.figure(figsize=(12,6))\n",
        "    sns.heatmap(cluster_means, cmap=\"viridis\", cbar=True)\n",
        "    plt.title(\"Cluster-wise Enzyme Behavior Heatmap\")\n",
        "    plt.show()\n",
        "\n",
        "    # PCA-based feature importance\n",
        "    pca_full = PCA()\n",
        "    pca_full.fit(X_scaled)\n",
        "    importance = np.abs(pca_full.components_[0])\n",
        "    feature_importance = pd.Series(importance, index=df.columns[:-1])\n",
        "    top_features = feature_importance.sort_values(ascending=False)[:15]\n",
        "    plt.figure(figsize=(8,4))\n",
        "    top_features.plot(kind=\"barh\")\n",
        "    plt.title(\"Top Contributors to Enzyme Pattern Separation\")\n",
        "    plt.show()\n",
        "\n",
        "    # PDF\n",
        "    rules = [f\"Cluster {c}:\\n\" + \"\\n\".join([f\"  {f} = {df[df.cluster==c][f].values[0]:.3f}\" for f in df.columns[:-1]]) for c in df.cluster.unique()]\n",
        "    pdf_path = \"Enzyme_Report.pdf\"\n",
        "    doc = SimpleDocTemplate(pdf_path)\n",
        "    styles = getSampleStyleSheet()\n",
        "    content = [Paragraph(\"<b>üß¨ EnzyVision-AI++ | Automated Enzyme Analysis</b>\",styles[\"Title\"]), Spacer(1,12)]\n",
        "    for r in rules:\n",
        "        content.append(Paragraph(r,styles[\"Normal\"]))\n",
        "        content.append(Spacer(1,6))\n",
        "    content.append(Image(sim_path, width=400, height=400))\n",
        "    doc.build(content)\n",
        "\n",
        "    return fig_3d, fig_heat, fig_radar, \"\\n\\n\".join(rules), pdf_path\n",
        "\n",
        "# ============================================================\n",
        "# 6Ô∏è‚É£ GRADIO APP\n",
        "# ============================================================\n",
        "ui = gr.Interface(\n",
        "    fn=run_pipeline_gradio,\n",
        "    inputs=gr.Image(type='pil', label=\"Upload Enzyme Activity Image\"),\n",
        "    outputs=[\n",
        "        gr.Plot(label=\"3D Latent Space\"),\n",
        "        gr.Plot(label=\"Feature Heatmap\"),\n",
        "        gr.Plot(label=\"Temporal Radar per Cluster\"),\n",
        "        gr.Textbox(label=\"All Feature Values per Cluster\"),\n",
        "        gr.File(label=\"PDF Report\")\n",
        "    ],\n",
        "    title=\"üß¨ EnzyVision-AI++ | Interactive Enzyme Analysis\",\n",
        "    description=\"Upload enzyme image ‚Üí feature extraction, clustering, autoencoder, temporal radar, similarity graph, PDF report\"\n",
        ")\n",
        "\n",
        "ui.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 698
        },
        "id": "du0OXhIrDzEu",
        "outputId": "ad53109f-dc24-44ff-e8c8-d6b9922c7b54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: reportlab in /usr/local/lib/python3.12/dist-packages (4.4.9)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.12/dist-packages (from reportlab) (11.3.0)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.12/dist-packages (from reportlab) (3.4.4)\n",
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://428d75a48614cd65ec.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://428d75a48614cd65ec.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    }
  ]
}